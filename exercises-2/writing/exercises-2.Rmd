---
title: "Exercises 2"
author: "Abby Johnson"
date: "3/4/2022"
output: md_document
---

```{r setup, include=FALSE}
# installs the librarian package if you don't have it
if (!("librarian" %in% rownames(utils::installed.packages()))) {
  utils::install.packages("librarian")
}

# put all of the packages that you import here
librarian::shelf( 
  cran_repo = "https://cran.microsoft.com/", # Dallas, TX
  ask = FALSE,
  stats, # https://stackoverflow.com/questions/26935095/r-dplyr-filter-not-masking-base-filter#answer-26935536
  here,
  kableExtra,
  rlang,
  ggthemes,
  tidyverse,
  janitor,
  magrittr,
  glue,
  lubridate,
  haven,
  snakecase,
  sandwich,
  lmtest,
  gganimate,
  gapminder,
  stargazer,
  snakecase,
  mosaicData,
  modelr,
  rsample,
  foreach,
  caret,
  parallel,
  purrr,
  pander,
  readr,
  xtable,
  gamlr,
  CVXR,
  pROC,
  ROCR,
  fastDummies
)

# tell here where we are so we can use it elsewhere
here::i_am("R/include.R")

capmetro_UT <- read_csv(here("data/capmetro_UT.csv"))
data(SaratogaHouses)
german_credit <- read_csv(here("data/german_credit.csv"))
hotels_dev <- read_csv(here("data/hotels_dev.csv"))
hotels_val <- read_csv(here("data/hotels_val.csv"))
```

```{r globaloptions, include=FALSE}
knitr::opts_chunk$set(fig.path = 'figures/')
```

<br>

### Exercises 2 
#### Abby Johnson
#### 3/7/22

<br>

### 1) Data visualization: UT CapMetro Ridership

<br>

```{r capmetro, include=FALSE}
capmetro_UT = mutate(capmetro_UT,
                     day_of_week = factor(day_of_week,
                                          levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
                     month = factor(month,
                                    levels=c("Sep", "Oct","Nov")))

mean_boardings = capmetro_UT %>%
  group_by(hour_of_day, day_of_week, month)%>%
  summarize(mean_boardings = mean(boarding))

mean_boardings_line = ggplot(mean_boardings) +
  geom_line(aes(x=hour_of_day, y=mean_boardings, color=month))+
  facet_wrap(~day_of_week)+
  labs(x="Hour",
       y="Average Boardings",
       title= "Average Boardings by Time",
       colour="Month")

capmetro_UT = mutate(capmetro_UT, minute = minute(timestamp))

boardings_per_hour = capmetro_UT %>%
  group_by(minute, hour_of_day, weekend, temperature)%>%
  summarize(boardings = sum(boarding))

boardings_per_hour_scatter = ggplot(boardings_per_hour) +
  geom_point(aes(temperature, boardings, color=weekend))+
  facet_wrap(~hour_of_day)+
  labs(x="Temperature",
       y="Boardings",
       title = "Boardings by Time and Temperature",
       colour="Weekend")

```


```{r capmetroplot1, echo=FALSE}
plot(mean_boardings_line)
```

From the figure above, we can see how UT CapMetro ridership changes by hour, day of week, and month (Sep, Oct, Nov). We can see that the peak boarding hour is about the same for all weekdays, but is much lower on the weekends. This makes sense when we consider UT class times, and students'demand (or lack-thereof) for a ride to campus on the weekends. Additionally, average boardings on Mondays in September are lower, which is likely due to the Monday university closure in observance of Labor Day in September. Moreover, we can see the average boardings on Wed/Thurs/Fri are lower in November for a similar reason. The university closes Wed/Thurs/Fri of Thanksgiving week, so there is likely to be almost no riders on the UT CapMetro routes for those days in November. 

<br>

```{r capmetroplot2, echo=FALSE}
plot(boardings_per_hour_scatter)
```

From the figure above, we can see how UT CapMetro ridership changes across temperature, hour of the day, and weekdays/weekends. When we hold hour of the day and weekend status constant, temperature does not seem to have a noticeable effect on student ridership. Within each hour of the day, we can see that ridership has a fairly flat correlation with temperature, which suggests there is no relationship between temperature and UT bus ridership. 

<br>

### 2) Saratoga House Prices

<br>

In order to build the best predictive model of house prices in Saratoga, NY, we can compare the performance of a linear model and a KNN model. Using various characteristics that describe the house and property, each model estimates what the price of an individual house should be. Then, each model's out-of-sample performance is cross-validated, in order to account random variation in the data. With these cross-validated out-of-sample performance measures, we can decide which model is better at estimating house price.
```{r saratoga, include=FALSE}
SaratogaHouses$waterfront <- ifelse(SaratogaHouses$waterfront == 'Yes', 1, 0)
SaratogaHouses$centralAir <- ifelse(SaratogaHouses$centralAir == 'Yes', 1, 0)
SaratogaHouses$newConstruction <- ifelse(SaratogaHouses$newConstruction == 'Yes', 1, 0)
SaratogaHouses$heating_electric <- ifelse(SaratogaHouses$heating == 'electric', 1, 0)
SaratogaHouses$heating_hotwater <- ifelse(SaratogaHouses$heating == 'hot water/steam', 1, 0)
SaratogaHouses$heating_hotair <- ifelse(SaratogaHouses$heating == 'hot air', 1, 0)
SaratogaHouses$fuel_electric <- ifelse(SaratogaHouses$fuel == 'electric', 1, 0)
SaratogaHouses$fuel_gas <- ifelse(SaratogaHouses$fuel == 'gas', 1, 0)
SaratogaHouses$fuel_oil <- ifelse(SaratogaHouses$fuel == 'oil', 1, 0)
SaratogaHouses$sewer_septic <- ifelse(SaratogaHouses$sewer == 'septic', 1, 0)
SaratogaHouses$sewer_public <- ifelse(SaratogaHouses$sewer == 'public/commercial', 1, 0)
SaratogaHouses$sewer_none <- ifelse(SaratogaHouses$sewer == 'none', 1, 0)

saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

K_folds = 10
SaratogaHouses = SaratogaHouses %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(SaratogaHouses)) %>% sample)

lm_rmse_cv = foreach(fold = 1:K_folds, .combine='c') %do% {
  lm = lm(price ~ (. -pctCollege - heating - heating_hotair - fuel - fuel_oil - sewer - sewer_none +(bedrooms*lotSize)),
  data=filter(SaratogaHouses, fold_id != fold))
modelr::rmse(lm, data=filter(SaratogaHouses, fold_id == fold))
}

lm_rmse_cv
mean(lm_rmse_cv)

SaratogaHouses_standardized = SaratogaHouses %>%
  mutate(price_s = scale(price), 
         lotSize_s = scale(lotSize),
         age_s = scale(age),
         landValue_s = scale(landValue),
         livingArea_s = scale(livingArea),
         pctCollege_s = scale(pctCollege),
         bedrooms_s = scale(bedrooms),
         fireplaces_s = scale(fireplaces),
         bathrooms_s = scale(bathrooms),
         rooms_s = scale(rooms),
         waterfront_s = scale(waterfront),
         newConstruction_s = scale(newConstruction),
         centralAir_s = scale(centralAir),
         heating_electric_s = scale(heating_electric),
         heating_hotwater_s = scale(heating_hotwater),
         heating_hotair_s = scale(heating_hotair),
         fuel_electric_s = scale(fuel_electric),
         fuel_gas_s = scale(fuel_gas),
         fuel_oil_s = scale(fuel_oil),
         sewer_septic_s = scale(sewer_septic),
         sewer_public_s = scale(sewer_public),
         sewer_none_s = scale(sewer_none))

saratoga_split_s = initial_split(SaratogaHouses_standardized, prop = 0.8)
saratoga_train_s = training(saratoga_split_s)
saratoga_test_s = testing(saratoga_split_s)

SaratogaHouses_standardized = SaratogaHouses_standardized %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(SaratogaHouses_standardized)) %>% sample)

knn_rmse_cv = foreach(fold = 1:K_folds, .combine='c') %do% {
  knn100 = knnreg(price ~(. -pctCollege - heating_hotair - fuel_oil - sewer_none),
                  data=filter(SaratogaHouses_standardized, fold_id != fold), k=20)
  modelr::rmse(knn100, data=filter(SaratogaHouses_standardized, fold_id == fold))
}

knn_rmse_cv
mean(knn_rmse_cv)

SaratogaHouses_standardized_folds = crossv_kfold(SaratogaHouses_standardized, k=K_folds)

k_grid = c(2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45,
           50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250, 300)

knn_cv_grid = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(SaratogaHouses_standardized_folds$train, ~ knnreg(price ~(. -pctCollege - heating_hotair - fuel_oil - sewer_none), k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, SaratogaHouses_standardized_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

knn_cv_grid_plot = ggplot(knn_cv_grid) + 
  geom_point(aes(x=k, y=err)) + 
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err)) + 
  scale_x_log10()
knn_cv_grid_plot
```

**Cross-validated RMSE for best linear model:**
```{r lm_rmse, echo=FALSE}
mean(lm_rmse_cv)
```

<br>

**Cross-validated RMSE for best KNN model:**
```{r knn_rmse, echo=FALSE}
mean(knn_rmse_cv)

```

<br>

We can see that the linear model proves to be better at predicting house price, because it has a significantly lower out-of-sample RMSE. The difference between these two RMSE estimates is about 11,800. This means that the linear model \$11,800 closer in their predictions of the true house price. This number is very large when you consider how much potential tax revenue comes from \$11,800. For example, if a local property tax rate is 1.5%, \$11,800 generates almost \$200 of revenue. Multiply that by just 2,000 homes in a county, and that becomes \$400,000 of tax revenue that is potentially over or under-estimated. As a local taxing authority, it is vital to mitigate this type of revenue estimation error, therefore, the linear model proves to be the best predictive choice.

<br>

### 3) Classification and retrospective sampling: German loan defaults

<br>

```{r credit, include=FALSE}
count_history = german_credit %>%
  group_by(history)%>%
  summarize(count=n())
count_history

count_history_plot = ggplot(count_history)+
  geom_col(aes(history, count))+
  labs(x="Credit History",
       y="Total Loans",
       title = "Total Loans by Credit History")
count_history_plot

count_default_history = german_credit %>%
  filter(Default==1)%>%
  group_by(history)%>%
  summarize(count=n())
count_default_history

count_default_history_plot = ggplot(count_default_history)+
  geom_col(aes(history, count))+
  labs(x="Credit History",
       y="Total Defaulted Loans",
       title = "Total Defaulted Loans by Credit History")
count_default_history_plot


default_by_history = german_credit %>%
  group_by(history)%>%
  summarize(prop_default = ( sum( Default == 1 ) / length( Default ) ))
default_by_history

prop_default_bar = ggplot(default_by_history) +
  geom_col(aes(x=history, y=prop_default))+
  labs( x="Credit History",
        y = "Probability of Default",
        title = "Default Probabilty by Credit History")
prop_default_bar

logit_default = glm(Default ~ duration + amount + installment + age + history + purpose + foreign, data=german_credit, family=binomial)
coef(logit_default) %>% round(2)

```

```{r credit_propplot, echo=FALSE}
plot(prop_default_bar)
```
From the figure above, we can see the calculated probability of loan default for each type of credit history. Those with 'good' credit history are far more likely to default on their loan than those with 'poor' or 'terrible' credit history. Moreover, those with 'terrible' credit history are the least likely to default on their loan.

<br>

```{r logit, echo=FALSE} 
logit_default %>% 
  tidy()%>%
  mutate(
    term = c("Intercept", "Duration", "Amount", "Installment", "Age", "Poor History", "Terrible History","Education","Goods/Repair","New Car","Used Car", "German"))%>%
  kable(
    caption = "**Logit Model Estimates For Predicting Default Probability**",
    col.names = c("Predictor", "Coefficient", "SE", "Z", "p"),
    digits = c(0, 2, 2, 2, 3),
    align = c("l", "r", "r", "r", "r")
  )
```
From the logit regression estimates above, we can see how credit history is estimated to effect the probability of loan default. Similar to the first bar plot, the logit coefficients tell us that individuals with 'poor' history are less likely to default than those with 'good' history, and those with 'terrible' history are the least likely to default of all credit history groups.  

<br>

Based on the results of this data, I do not think this data set is appropriate for building a predictive model of defaults. Individuals with 'good' credit histories should be the least likely to default on their loans, while those with 'terrible' history should be the most likely to default on their loans. Individuals with 'good' credit are more likely to be offered a loan in the first place, which means there inevitably were more loans with 'good' credit history in the retrospective data collection. This means that 'good' credit loans will have artificially high default rates, and 'terrible' credit loans will have artificially low default rates. Additionally, there was no randomization in the default status of the retrospective data collection, which eliminates any natural variation. 

<br>

In an effort to improve the bank's sampling scheme, I would recommend taking a large random sample of loans from the bank's overall portfolio and bootstrapping the sample in order to get more accurate estimates of loan default probability. Therefore, the bank won't over-sample defaults with 'good' history and maintain random variation. 

<br>

### 4) Model building and validation: Children and hotel reservations

<br>

```{r validation, include=FALSE}
hotels_dev <- read_csv(here("data/hotels_dev.csv"))
hotels_dev = dummy_cols(hotels_dev) %>%
  mutate(reserved_room_type_I=0)%>%
  mutate(reserved_room_type_J=0)%>%
  mutate(reserved_room_type_K=0)%>%
  mutate(reserved_room_type_L=0)%>%
  mutate(assigned_room_type_J=0)%>%
  mutate(assigned_room_type_L=0)%>%
  select(-c(assigned_room_type, reserved_room_type, hotel, meal, market_segment, distribution_channel, deposit_type, customer_type, required_car_parking_spaces))

hotels_val <- read_csv(here("data/hotels_val.csv"))
hotels_val = dummy_cols(hotels_val) %>%
  mutate(reserved_room_type_I=0)%>%
  mutate(reserved_room_type_J=0)%>%
  mutate(reserved_room_type_K=0)%>%
  mutate(reserved_room_type_L=0)%>%
  mutate(assigned_room_type_J=0)%>%
  mutate(assigned_room_type_L=0)%>%
  select(-c(assigned_room_type, reserved_room_type, hotel, meal, market_segment, distribution_channel, deposit_type, customer_type, required_car_parking_spaces))

#m1_x= model.matrix(children ~ market_segment_Aviation + market_segment_Complementary + market_segment_Corporate + market_segment_Direct + market_segment_Groups + market_segment_Online_TA + adults + customer_type_Contract + customer_type_Group + customer_type_Transient + customer_type_Transient-Party + is_repeated_guest, data=hotels_dev)
m2_x= model.matrix(children ~ (. - arrival_date -1), data=hotels_dev)
#m3_x= model.matrix(children ~ (. -arrival_date - 1 + market_segment_Corporate*adults), data=hotels_dev)
y = hotels_dev$children

#m1_lasso = cv.gamlr(m1_x, y, nfold=10, family="binomial")
#m1_plot = plot(m1_lasso, bty="n")
#log(m1_lasso$lambda.min)

m2_lasso = cv.gamlr(m2_x, y, nfold=10, family="binomial")
plot(m2_lasso, bty="n")
log(m2_lasso$lambda.min)

#m3_lasso = cv.gamlr(m3_x, y, nfold=10, family="binomial") # Best linear model based out-of-sample deviance
#plot(m3_lasso, bty="n") 
#log(m3_lasso$lambda.min)

#########################

val_x= model.matrix(children ~ (. - arrival_date -1), data=hotels_val)

lasso_pred = predict(m2_lasso, val_x, select= "min")
pred = predict(m2_lasso, val_x, type= "response")

yhat_val = ifelse(pred >= 0.5, 1, 0)

conf = table(y=hotels_val$children, yhat=yhat_val)
conf[2,2] / (conf[2,1] + conf[2,2]) #TPR
conf[1,2] / (conf[1,1] + conf[1,2]) #FPR

TPR = 139/(263+139)
TPR

FPR = 57/(4540+57)
FPR

roc_curve = plot(roc(hotels_val$children, pred))

#########################

K_folds = 20
hotels_val = hotels_val%>%
  mutate(fold_id = rep(1:K_folds, length=nrow(hotels_val)) %>% sample)

folds_lasso = foreach(fold = 1:K_folds, .combine='c') %do% {
  in_fold_data = filter(hotels_val, fold_id == fold)
  out_fold_data = filter(hotels_val, fold_id != fold)
  x=model.matrix(children ~ (. -arrival_date - 1 ), data=out_fold_data)
  y=out_fold_data$children
  lasso = cv.gamlr(x, y,nfold=5, family="binomial")
  xval=model.matrix(children ~ (. -arrival_date - 1), data=in_fold_data)
  pred = predict(lasso, xval, type= "response")
  yhat_val = ifelse(pred >= 0.5, 1, 0)
  table = table(y=in_fold_data$children, yhat=yhat_val)
  TPR = table[2,2] / (table[2,1] + table[2,2]) #TPR
}


folds_lasso = data.frame(folds_lasso)
colnames(folds_lasso)=c("TPR")
folds_lasso$folds = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
folds_lasso

TPR_fold_comparison = ggplot(folds_lasso)+
  geom_col(aes(x=folds, y=TPR))+labs(
    x="Fold",
    y="TPR",
    title = "TPR by Fold")
TPR_fold_comparison

mean_TPR = folds_lasso %>%
  summarize(meanTPR = mean(TPR))
```

<br>

**Compare Out-of-Sample Performance**

<br>

*Best Linear Model*
```{r best_linear, echo=FALSE}
plot(m2_lasso, bty="n")
```

<br>

Using deviance as a measure of out-of-sample performance, the best linear model achieves a binomial deviance lower than the baseline 1 model, and marginally lower than the baseline 2 model. 

<br>

#### Model Validation: Step 1

<br>

Using the best linear model and the validation data set, I calculated the True Positive Rate (Sensitivity) and the False Positive Rate (Specificity) for the predictions of the children variable. With these two measurements, I created a ROC curve as seen below. 

<br>

```{r validation_2, echo=FALSE}
plot(roc_curve)
```

<br>

#### Model Validation: Step 2

<br>

After creating 20 folds in the validation data set, I predicted whether each booking would have children on it, and then estimated the total expected number of bookings with children for each fold. Then, I compared these predictions with the actual number of bookings with children for each fold, and calculated the True Positive Rate (TRP). 

<br>

The mean TPR across all folds was about 25%. This means that among the bookings with children on it, the model correctly predicted 25% as having children. The plot below summarizes the TRP for all 20 folds. We can see that the TRP ranges from a little less than 10% up to about 50%.
```{r validation_3, echo=FALSE}
mean_TPR
plot(TPR_fold_comparison)
```

<br>


